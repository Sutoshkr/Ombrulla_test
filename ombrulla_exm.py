# -*- coding: utf-8 -*-
"""Ombrulla exm.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19S09OiIt0F91HR8KmW-eaXmbJy0rf5KJ
"""

pip install torch torchvision torchaudio

pip install ultralytics

from ultralytics import YOLO

def detect_objects(image_path):
  model=YOLO("yolov8n.pt")
  result = model(image_path)
  objects=[]
  for r in result:
    boxes=r.boxes
    for box in boxes:
      cls_id = int(box.cls[0])
      conf=float(box.conf[0])
      label=model.names[cls_id]
      objects.append({"Name":label, "Confidence score":conf})

  return objects

pip install transformers

from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

def generate_response(objects):
    # Extract object names
    object_names = list(set([obj["Name"] for obj in objects]))
    prompt_input = f"Describe a scene that includes the following objects: {', '.join(object_names)}. "

    # Load LLM
    tokenizer = AutoTokenizer.from_pretrained("distilgpt2")
    model = AutoModelForCausalLM.from_pretrained("distilgpt2")

    inputs = tokenizer.encode(prompt_input, return_tensors="pt")
    outputs = model.generate(inputs, max_length=50, do_sample=True, top_k=50)

    generated_prompt = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return generated_prompt

#main section

def main():
  image_path="/content/object.jpg"
  prompt= input("Enter a prompt:")

  objects=detect_objects(image_path)
  print(objects)
  print("\nGenerate response\n")
  response=generate_response(objects)
  print(response)

if __name__=="__main__":
  main()

#there was error while running hugging face model